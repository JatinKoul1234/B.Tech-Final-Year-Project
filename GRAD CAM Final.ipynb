{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f1f7b9f-d76a-4c40-90ce-0ff59be176a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50  # Or your specific ResNet variant\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c2b4d0-cb5d-4447-a942-e38733f8ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Define Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f70b37d-2132-45b3-86f6-b4afd29fe9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from trained_model.h5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "h5_file_path = 'trained_model.h5'  # Replace with the actual path to your .h5 file\n",
    "try:\n",
    "    model = tf.keras.models.load_model(h5_file_path)\n",
    "    print(\"Model loaded successfully from {}\".format(h5_file_path))\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Model file not found at {}\".format(h5_file_path))\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(\"Error loading model: {}\".format(e))\n",
    "    print(e)  # Print the full exception for debugging\n",
    "    exit()# --- 2. Instantiate Your ResNet Model Architecture ---\n",
    "model = resnet50(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7149448-98f5-4875-8890-c051f31d3492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# --- 1. Define Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torchvision.models import resnet50\n",
    "# --- 2. Instantiate Your ResNet Model Architecture ---\n",
    "model = resnet50(pretrained=False)\n",
    "# --- 4. Set Model to Evaluation Mode ---\n",
    "model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b297d41-cd25-4ff8-96f5-556865e53934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e27229a1-6d39-491d-ae1a-ca74262c4978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define Image Transformations (Match Your Training) ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust if your input size was different\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization if applicable\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dec74371-e49b-4492-b0c7-abfdc3d633da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b73f5b3-abe9-4683-884a-057e800aac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# --- 1. Define Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# --- 5. Define Image Transformations (Match Your Training) ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Adjust if your input size was different\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization if applicable\n",
    "])\n",
    "# --- 6. Load and Preprocess the Image for Explanation ---\n",
    "image_path = \"D:/.vscode/My Major Project/WhatsApp Image 2025-05-03 at 12.44.31_015839af.jpg\"  # Replace with the path to your image\n",
    "try:\n",
    "    original_image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(original_image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Image not found at {image_path}\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading image: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef952e00-0a9e-496c-a7d0-fcddcaa7f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50  # Or your specific ResNet variant\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "# --- 2. Instantiate Your ResNet Model Architecture ---\n",
    "model = resnet50(pretrained=False)\n",
    "target_layer = model.layer4[-1].conv3  # Try this for ResNet50 initially\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b49b40c8-c421-42f0-bd88-93f53b3bd9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = GradCAM(model=model, target_layers=[target_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed7e2c36-ea91-4acb-9a2d-1134af99b243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class index: 27\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Get Model Prediction (Optional, but useful for targeting a specific class) ---\n",
    "output = model(input_tensor)\n",
    "_, predicted_class = torch.max(output, 1)\n",
    "predicted_class_index = predicted_class.item()\n",
    "print(f\"Predicted class index: {predicted_class_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c22b9ef-f779-427f-8441-2b6730030a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56bfdf9f-5e34-473c-9038-4c37990c2e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 10. Generate the Grad-CAM Heatmap ---\n",
    "# You can target a specific class or use the predicted class\n",
    "targets = [ClassifierOutputTarget(predicted_class_index)]  # Create the targets list\n",
    "# specific_target_class_index = your_target_class_index\n",
    "# targets = [ClassifierOutputTarget(specific_target_class_index)]\n",
    "\n",
    "grayscale_cam = cam(input_tensor=input_tensor, targets=targets)  # Use 'targets'\n",
    "grayscale_cam = grayscale_cam[0, :]  # Remove batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ed8f886-8247-4da7-b3b3-444fc1a3fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 11. Visualize the Heatmap on the Original Image ---\n",
    "original_image_np = np.array(original_image)[:, :, ::-1]  # Convert RGB to BGR for OpenCV\n",
    "\n",
    "# Resize the grayscale_cam to the original image size\n",
    "resized_cam = cv2.resize(grayscale_cam, (original_image_np.shape[1], original_image_np.shape[0]))\n",
    "\n",
    "visualization = show_cam_on_image(original_image_np / 255.0, resized_cam, use_rgb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c6bf236-8010-469c-8e39-ebb5b4270711",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GradCAM' object has no attribute 'remove_hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# cv2.imwrite(\"gradcam_output.jpg\", visualization)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# --- 13. Clean Up ---\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mcam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_hook\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GradCAM' object has no attribute 'remove_hook'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50  # Or your specific ResNet variant\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# --- 11. Visualize the Heatmap on the Original Image ---\n",
    "original_image_np = np.array(original_image)[:, :, ::-1]  # Convert RGB to BGR for OpenCV\n",
    "resized_cam = cv2.resize(grayscale_cam, (original_image_np.shape[1], original_image_np.shape[0]))\n",
    "visualization = show_cam_on_image(original_image_np / 255.0, resized_cam, use_rgb=True)\n",
    "\n",
    "import cv2\n",
    "# --- 12. Display or Save the Visualization ---\n",
    "cv2.imshow(\"Grad-CAM Visualization\", visualization)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "# cv2.imwrite(\"gradcam_output.jpg\", visualization)\n",
    "\n",
    "# --- 13. Clean Up ---\n",
    "cam.remove_hook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24077ed7-bc8b-47ec-a9ea-d1274d756c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
